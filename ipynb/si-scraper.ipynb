{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI Scraper\n",
    "\n",
    "This notebook scrapes data from [Siamensis SI](http://www.siamensis.org/species_index).\n",
    "\n",
    "## Scraping\n",
    "\n",
    "- Get a data object from Siamensis SI.\n",
    "- Loop through all children nodes recursively.\n",
    "- Get `id`, `num_children`, `children_id` from each node and store as a list.\n",
    "- Download `html` file of each `id` into `../node` folder.\n",
    "\n",
    "## Parsing\n",
    "\n",
    "- Loop through each item in a scraped list.\n",
    "- Parse each `html` page and store parsed data in a dict.\n",
    "- Save this parsed data as `json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import html\n",
    "from os import path, pardir, mkdir\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from dateutil import parser\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'attr', 'mlid', 'num_children', 'children'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get whole tree from endpoint\n",
    "r = requests.get('http://www.siamensis.org/json?type=tree')\n",
    "# get json from request\n",
    "si_json = r.json()[0][0]\n",
    "# show keys of json obj\n",
    "si_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siDatetimeParser(date_str):\n",
    "    # if date str is in English just send to normal parser\n",
    "    if re.match(r'.*[a-zA-Z].*', date_str):\n",
    "        return parser.parse(date_str)\n",
    "    \n",
    "    # if not split and clean then send to parse as Thai year\n",
    "    date_tuple = tuple(filter(lambda x: x != '', date_str.split(' ')))\n",
    "    \n",
    "    # month mapper\n",
    "    months = {\n",
    "        'Jan': ['มค', 'มกราคม', 'มกรา'],\n",
    "        'Feb': ['กพ', 'กุมภาพันธ์', 'กุมภา'],\n",
    "        'Mar': ['มีค', 'มีนาคม', 'มีนา'],\n",
    "        'Apr': ['เมย', 'เมษายน', 'เมษา'],\n",
    "        'May': ['พค', 'พฤษภาคม', 'พฤษภา'],\n",
    "        'Jun': ['มิย', 'มิถุนายน', 'มิถุนา'],\n",
    "        'Jul': ['กค', 'กรกฎาคม', 'กรกฎา'],\n",
    "        'Aug': ['สค', 'สิงหาคม', 'สิงหา'],\n",
    "        'Sep': ['กย', 'กันยายน', 'กันยา'],\n",
    "        'Oct': ['ตค', 'ตุลาคม', 'ตุลา'],\n",
    "        'Nov': ['พย', 'พฤศจิกายน', 'พฤศจิกา'],\n",
    "        'Dec': ['ธค', 'ธันวาคม', 'ธันวา']\n",
    "    }\n",
    "    \n",
    "    # strip possible artifacts like dot and space\n",
    "    month_strip = date_tuple[1].replace('.', '').replace(' ', '')\n",
    "    year_strip = date_tuple[2]\n",
    "    \n",
    "    # roughly check if this is really buddhist calendar\n",
    "    # if not return the input year\n",
    "    if int(year_strip) > dt.now().year:\n",
    "        # this is most likely a buddhist calendar in the present time\n",
    "        yc = str(int(year_strip) - 543)\n",
    "    else:\n",
    "        yc = year_strip\n",
    "        \n",
    "    # this is real risky because no way to detect error\n",
    "    # but just skip it for now :d\n",
    "    for m in months.keys():\n",
    "        if month_strip in months[m]:\n",
    "            mc = m\n",
    "            break\n",
    "            \n",
    "    return parser.parse(' '.join([date_tuple[0], mc, yc]), dayfirst=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get ids of all children in a list\n",
    "def idGetter(children_ls):\n",
    "    ids = []\n",
    "    for child in children_ls:\n",
    "        ids.append(child['attr']['link'].split('/')[-1])\n",
    "    return ids\n",
    "\n",
    "# function to scrapte data in the object recursively\n",
    "def scraper(obj, keeper=[]):\n",
    "    # each item is stored in dict\n",
    "    item_dict = dict()\n",
    "    # loop through keys in the object\n",
    "    for key in obj.keys():\n",
    "        # take attr link as an id and put in dict\n",
    "        if key == 'attr':\n",
    "            link_id = obj[key]['link'].split('/')[-1]\n",
    "            # print(f\"getting data of node id: {link_id}..\")\n",
    "            item_dict['id'] = link_id\n",
    "        # get ids of children and count and put in dict\n",
    "        elif key == 'children':\n",
    "            all_ids = idGetter(obj[key])\n",
    "            item_dict['num_children'] = len(all_ids)\n",
    "            item_dict['children_ids'] = all_ids\n",
    "            for item in obj[key]:\n",
    "                # then scrape each children object with scraper\n",
    "                # this will do recursively until no more obj\n",
    "                scraper(item, keeper)\n",
    "    # store each item dict in a keeper\n",
    "    keeper.append(item_dict)\n",
    "    # and return when all is done\n",
    "    return keeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_node = scraper(si_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6510/6510 [1:29:47<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# save all extracted_node as html per node\n",
    "# this cell can take as long as 1.5 hrs, you can skip and use files in ../node\n",
    "for node in tqdm(extracted_node):\n",
    "    url = f'http://www.siamensis.org/species_index/node/{node[\"id\"]}'\n",
    "    r = requests.get(url)\n",
    "    save_path = f'../node/{node[\"id\"]}.html'\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = glob('../node/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_extracted_node = deepcopy(extracted_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE at: 6510\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "si_collection = []\n",
    "\n",
    "for each in copied_extracted_node:\n",
    "    # pre-allocate object\n",
    "    si_node = {\n",
    "        'id': '',\n",
    "        'rank': '',\n",
    "        'title': '',\n",
    "        'author': '',\n",
    "        'timestamp': '',\n",
    "        'description': '',\n",
    "        'images': [],\n",
    "        'parents': [],\n",
    "    }\n",
    "    \n",
    "    # add id\n",
    "    si_node['id'] = each['id']\n",
    "    \n",
    "    file_name = f\"../node/{each['id']}.html\"\n",
    "    \n",
    "    soup = BeautifulSoup(open(file_name, \"r\"), \"html.parser\")\n",
    "\n",
    "    # rank and rank name\n",
    "    tmp = soup.select('.node-title')[0].text\n",
    "    \n",
    "    main_rank_pt = r'^[\\s ฺฺ]*([a-zA-Z]+)\\s*'\n",
    "    rank_pt = re.compile(fr'{main_rank_pt}\\:')\n",
    "    no_colon_pt = re.compile(fr'{main_rank_pt}')\n",
    "    try:\n",
    "        rank = rank_pt.search(tmp).group(1)\n",
    "    except:\n",
    "        try:\n",
    "            rank = no_colon_pt.search(tmp).group(1)\n",
    "#             print(rank)\n",
    "        except:\n",
    "            print('---- exception is throw at [rank]:')\n",
    "            print(each)\n",
    "            print('---- prettified')\n",
    "            print(soup.prettify())\n",
    "            break\n",
    "    # assign to a node\n",
    "    si_node['rank'] = rank\n",
    "    \n",
    "    main_title_pt = r'([\\(\\)a-zA-ZÀ-ÖØ-öø-ÿĀ-ž \\-,&0-9\\.=ก-๙\\[\\]\\']+)\\s*$'\n",
    "    title_pt = re.compile(fr':\\s*{main_title_pt}')\n",
    "    no_colon_pt = re.compile(fr'^\\s*[a-zA-ZÀ-ÖØ-öø-ÿĀ-ž]+ {main_title_pt}')\n",
    "    try:\n",
    "        title = title_pt.search(tmp).group(1)\n",
    "    except:\n",
    "        try:\n",
    "            title = no_colon_pt.search(tmp).group(1)\n",
    "#             print(title)\n",
    "        except:\n",
    "            print('---- exception is throw at [title]:')\n",
    "            print(each)\n",
    "            print('---- prettified')\n",
    "            print(tmp)\n",
    "            print(soup.prettify())\n",
    "            break\n",
    "    # assign to a node\n",
    "    si_node['title'] = title\n",
    "    \n",
    "    # author, timestamp and modified\n",
    "    tmp = soup.select('.node-submitted')[0].text\n",
    "    tmp = tmp.replace('(IP:  )', '')\n",
    "    \n",
    "    author_pt = re.compile(r'.*เขียนโดย (.*) เมื่อ.*')\n",
    "    author = author_pt.search(tmp).group(1)\n",
    "    \n",
    "    timestamp_pt = re.compile(r'.*เมื่อ (.*)$')\n",
    "    \n",
    "    try:\n",
    "        timestamp = siDatetimeParser(timestamp_pt.search(tmp).group(1))\n",
    "    except:\n",
    "        print('get:', timestamp_pt.search(tmp).group(1))\n",
    "        print('actual:', tmp)\n",
    "    \n",
    "    # assign to a node\n",
    "    si_node['author'] = author\n",
    "    si_node['timestamp'] = timestamp.strftime(\"%d %b %Y, %H:%M\")\n",
    "    \n",
    "#     print(author)\n",
    "#     print(timestamp)\n",
    "#     print(siDatetimeParser(timestamp))\n",
    "    \n",
    "    # content, example images\n",
    "    # content is anything between </div> of <div class=\"node-header\">\n",
    "    # to before <div class=\"node-submitted\">, this could be tricky\n",
    "    # images are also included here but the full link is a thumbnail\n",
    "    # full size image is available in the href of a wrapping img\n",
    "    # and the link is relative\n",
    "    \n",
    "    # TODOs.\n",
    "    # extract all content and filter images out\n",
    "    # get only image uri in href and prepend with full link\n",
    "    # content should be converted to MD and store in 'description'\n",
    "    # images should be stored in 'images'\n",
    "    \n",
    "    tmp = soup.select('.node-header')[0].next_siblings\n",
    "    content = ''\n",
    "    images = []\n",
    "    \n",
    "    for tag in tmp:\n",
    "        try:\n",
    "            attrs_keys = tag.attrs.keys()\n",
    "            \n",
    "            if tag.name == 'div':\n",
    "                if 'class' in attrs_keys:\n",
    "    #             print(tag.attrs.keys(), tag.attrs['class'], tag.attrs['class'][0] == 'node-submitted')\n",
    "                    if tag.attrs['class'][0] == 'node-submitted':\n",
    "#                         print(tag, 'break')\n",
    "                        break\n",
    "                if 'id' in attrs_keys:\n",
    "                    # additional images\n",
    "                    if tag.attrs['id'] == 'jstree_thumb':\n",
    "                        # extract image url from within href of a\n",
    "                        img_links = [f'http://www.siamensis.org{x[\"href\"]}' for x in tag.find_all('a', attrs={'class': 'si-image'})]\n",
    "                        # caption is from alt of img \n",
    "                        captions = [f'{x[\"alt\"]}' for x in tag.find_all('img', attrs={'class': 'image-item'})]\n",
    "                        # construct image array with obj of images\n",
    "                        images = [{\n",
    "                            'url': k,\n",
    "                            'caption': v\n",
    "                        } for k,v in zip(img_links, captions)]\n",
    "                        \n",
    "#                         print(images)\n",
    "                        break\n",
    "            try:\n",
    "                content += tag.prettify()\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    # see how content is\n",
    "#     print(content)\n",
    "                                    \n",
    "    # assign to a node\n",
    "    si_node['description'] = content\n",
    "    si_node['images'] = images\n",
    "    \n",
    "    # list of parents\n",
    "    tmp = soup.find(attrs={'type': 'text/javascript'}).text\n",
    "    parent_pt = re.compile(r\"jQ.parseJSON\\('(.*)'\\).*\")  \n",
    "    parent_list = parent_pt.search(tmp).group(1)\n",
    "    parent_list = ast.literal_eval(parent_list)\n",
    "    parent_list = [x.strip() for x in parent_list]\n",
    "    parent_list.remove(each['id'])                                \n",
    "#     print(parent_list)\n",
    "    \n",
    "    # assign to a node\n",
    "    si_node['parents'] = parent_list\n",
    "                                    \n",
    "    # add to list and count success \n",
    "    si_collection.append(si_node)\n",
    "    counter += 1\n",
    "                                    \n",
    "print(f'DONE at: {counter}')\n",
    "#     >>> x = ast.literal_eval(x)\n",
    "#     >>> x\n",
    "#     ['A', 'B', 'C', ' D']\n",
    "#     >>> x = [n.strip() for n in x]\n",
    "        \n",
    "    \n",
    "#     if content:\n",
    "# #         print(content)\n",
    "# #         print('\\n---------\\n')\n",
    "#         counter+=1\n",
    "        \n",
    "#     print(tmp)\n",
    "#     print(soup.prettify())\n",
    "#     print(each)\n",
    "#     print(si_node)\n",
    "#     print('\\n---------\\n')\n",
    "    \n",
    "    \n",
    "#     if counter >= 20:\n",
    "#         break\n",
    "# #     counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '6183',\n",
       " 'rank': 'Phylum',\n",
       " 'title': 'Crenarchaeota',\n",
       " 'author': 'lepton',\n",
       " 'timestamp': '11 Mar 2011, 12:15',\n",
       " 'description': '',\n",
       " 'images': [],\n",
       " 'parents': ['673', '708', '710']}"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si_collection[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = path.join(path.pardir, 'si-data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'w') as f:\n",
    "    json.dump(si_collection, f, sort_keys=True, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
