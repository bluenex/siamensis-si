{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI Scraper\n",
    "\n",
    "This notebook scrapes data from [Siamensis SI](http://www.siamensis.org/species_index).\n",
    "\n",
    "## Scraping\n",
    "\n",
    "- Get a data object from Siamensis SI.\n",
    "- Loop through all children nodes recursively.\n",
    "- Get `id`, `num_children`, `children_id` from each node and store as a list.\n",
    "- Download `html` file of each `id` into `../node` folder. (this step may take a while)\n",
    "\n",
    "## Parsing\n",
    "\n",
    "- Loop through each item in a scraped list.\n",
    "- Parse each `html` page and store parsed data in a dict.\n",
    "- Save this parsed data as `json`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import ast\n",
    "import html\n",
    "from os import path, pardir, mkdir\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from dateutil import parser\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get ids of all children in a list\n",
    "def idGetter(children_ls):\n",
    "    ids = []\n",
    "    for child in children_ls:\n",
    "        ids.append(child['attr']['link'].split('/')[-1])\n",
    "    return ids\n",
    "\n",
    "# function to scrapte data in the object recursively\n",
    "def scraper(obj, keeper=[]):\n",
    "    # each item is stored in dict\n",
    "    item_dict = dict()\n",
    "    # loop through keys in the object\n",
    "    for key in obj.keys():\n",
    "        # take attr link as an id and put in dict\n",
    "        if key == 'attr':\n",
    "            link_id = obj[key]['link'].split('/')[-1]\n",
    "            # print(f\"getting data of node id: {link_id}..\")\n",
    "            item_dict['id'] = link_id\n",
    "        # get ids of children and count and put in dict\n",
    "        elif key == 'children':\n",
    "            all_ids = idGetter(obj[key])\n",
    "            item_dict['num_children'] = len(all_ids)\n",
    "            item_dict['children_ids'] = all_ids\n",
    "            for item in obj[key]:\n",
    "                # then scrape each children object with scraper\n",
    "                # this will do recursively until no more obj\n",
    "                scraper(item, keeper)\n",
    "    # store each item dict in a keeper\n",
    "    keeper.append(item_dict)\n",
    "    # and return when all is done\n",
    "    return keeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any step need to run this cell first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'attr', 'mlid', 'num_children', 'children'])"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get whole tree from endpoint\n",
    "r = requests.get('http://www.siamensis.org/json?type=tree')\n",
    "# get json from request\n",
    "si_json = r.json()[0][0]\n",
    "# extract node from si_json big object\n",
    "extracted_node = scraper(si_json)\n",
    "# show keys of json obj\n",
    "si_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6510/6510 [1:29:47<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# save all extracted_node as html per node\n",
    "# this cell can take as long as 1.5 hrs, you can skip and use files in ../node\n",
    "for node in tqdm(extracted_node):\n",
    "    url = f'http://www.siamensis.org/species_index/node/{node[\"id\"]}'\n",
    "    r = requests.get(url)\n",
    "    save_path = f'../node/{node[\"id\"]}.html'\n",
    "    \n",
    "    with open(save_path, 'w') as f:\n",
    "        f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siDatetimeParser(date_str):\n",
    "    # if date str is in English just send to normal parser\n",
    "    if re.match(r'.*[a-zA-Z].*', date_str):\n",
    "        return parser.parse(date_str)\n",
    "    \n",
    "    # if not split and clean then send to parse as Thai year\n",
    "    date_tuple = tuple(filter(lambda x: x != '', date_str.split(' ')))\n",
    "    \n",
    "    # month mapper\n",
    "    months = {\n",
    "        'Jan': ['มค', 'มกราคม', 'มกรา'],\n",
    "        'Feb': ['กพ', 'กุมภาพันธ์', 'กุมภา'],\n",
    "        'Mar': ['มีค', 'มีนาคม', 'มีนา'],\n",
    "        'Apr': ['เมย', 'เมษายน', 'เมษา'],\n",
    "        'May': ['พค', 'พฤษภาคม', 'พฤษภา'],\n",
    "        'Jun': ['มิย', 'มิถุนายน', 'มิถุนา'],\n",
    "        'Jul': ['กค', 'กรกฎาคม', 'กรกฎา'],\n",
    "        'Aug': ['สค', 'สิงหาคม', 'สิงหา'],\n",
    "        'Sep': ['กย', 'กันยายน', 'กันยา'],\n",
    "        'Oct': ['ตค', 'ตุลาคม', 'ตุลา'],\n",
    "        'Nov': ['พย', 'พฤศจิกายน', 'พฤศจิกา'],\n",
    "        'Dec': ['ธค', 'ธันวาคม', 'ธันวา']\n",
    "    }\n",
    "    \n",
    "    # strip possible artifacts like dot and space\n",
    "    month_strip = date_tuple[1].replace('.', '').replace(' ', '')\n",
    "    year_strip = date_tuple[2]\n",
    "    \n",
    "    # roughly check if this is really buddhist calendar\n",
    "    # if not return the input year\n",
    "    if int(year_strip) > dt.now().year:\n",
    "        # this is most likely a buddhist calendar in the present time\n",
    "        yc = str(int(year_strip) - 543)\n",
    "    else:\n",
    "        yc = year_strip\n",
    "        \n",
    "    # this is real risky because no way to detect error\n",
    "    # but just skip it for now :d\n",
    "    for m in months.keys():\n",
    "        if month_strip in months[m]:\n",
    "            mc = m\n",
    "            break\n",
    "            \n",
    "    return parser.parse(' '.join([date_tuple[0], mc, yc]), dayfirst=True)\n",
    "    \n",
    "def get_id(raw_node):\n",
    "    return raw_node['id']\n",
    "\n",
    "def get_rank_and_title(node_soup):\n",
    "    # select rank and title tag\n",
    "    tmp = node_soup.select('.node-title')[0].text\n",
    "    \n",
    "    # get rank first\n",
    "    main_rank_pt = r'^[\\s ฺฺ]*([a-zA-Z]+)\\s*'\n",
    "    rank_pt = re.compile(fr'{main_rank_pt}\\:')\n",
    "    no_colon_pt = re.compile(fr'{main_rank_pt}')\n",
    "    try:\n",
    "        rank = rank_pt.search(tmp).group(1)\n",
    "    except:\n",
    "        try:\n",
    "            rank = no_colon_pt.search(tmp).group(1)\n",
    "            # print(rank)\n",
    "        except:\n",
    "            print('---- exception is throw at [rank]:')\n",
    "            print(each)\n",
    "            print('---- prettified')\n",
    "            print(node_soup.prettify())\n",
    "            # return None so the outer loop breaks\n",
    "            return None\n",
    "    \n",
    "    # if rank passes get title\n",
    "    main_title_pt = r'([\\(\\)a-zA-ZÀ-ÖØ-öø-ÿĀ-ž \\-,&0-9\\.=ก-๙\\[\\]\\']+)\\s*$'\n",
    "    title_pt = re.compile(fr':\\s*{main_title_pt}')\n",
    "    no_colon_pt = re.compile(fr'^\\s*[a-zA-ZÀ-ÖØ-öø-ÿĀ-ž]+ {main_title_pt}')\n",
    "    try:\n",
    "        title = title_pt.search(tmp).group(1)\n",
    "    except:\n",
    "        try:\n",
    "            title = no_colon_pt.search(tmp).group(1)\n",
    "            # print(title)\n",
    "        except:\n",
    "            print('---- exception is throw at [title]:')\n",
    "            print(each)\n",
    "            print('---- prettified')\n",
    "            print(tmp)\n",
    "            print(node_soup.prettify())\n",
    "            # return None so the outer loop breaks\n",
    "            return None\n",
    "    # if all pass, return properly\n",
    "    return (rank, title)\n",
    "\n",
    "def get_author_and_timestamp(node_soup):\n",
    "    # select tag where author and timestamp live\n",
    "    tmp = node_soup.select('.node-submitted')[0].text\n",
    "    # some of the datetime is appended with this abnormal text\n",
    "    tmp = tmp.replace('(IP:  )', '')\n",
    "    \n",
    "    author_pt = re.compile(r'.*เขียนโดย (.*) เมื่อ.*')\n",
    "    author = author_pt.search(tmp).group(1)\n",
    "    \n",
    "    timestamp_pt = re.compile(r'.*เมื่อ (.*)$')\n",
    "    \n",
    "    try:\n",
    "        timestamp = siDatetimeParser(timestamp_pt.search(tmp).group(1))\n",
    "    except:\n",
    "        print('get:', timestamp_pt.search(tmp).group(1))\n",
    "        print('actual:', tmp)\n",
    "        # return None so the outer loop breaks\n",
    "        return None\n",
    "    \n",
    "    return (author, timestamp)\n",
    "\n",
    "def get_description_and_images(node_soup):\n",
    "    # get all tags after node-header class\n",
    "    # this is where description and images live.. in HTML\n",
    "    tmp = node_soup.select('.node-header')[0].next_siblings\n",
    "    # declare empty content and images to append later\n",
    "    content = ''\n",
    "    images = []\n",
    "    \n",
    "    # loop through each tag\n",
    "    for tag in tmp:\n",
    "        try:\n",
    "            # try getting attributes keys\n",
    "            attrs_keys = tag.attrs.keys()\n",
    "            \n",
    "            if tag.name == 'div':\n",
    "                # if it is a class check if it is the last tag of content\n",
    "                if 'class' in attrs_keys:\n",
    "                    # print(tag.attrs.keys(), tag.attrs['class'], tag.attrs['class'][0] == 'node-submitted')\n",
    "                    if tag.attrs['class'][0] == 'node-submitted':\n",
    "                        # print(tag, 'break')\n",
    "                        break\n",
    "                # if it is an id check if it is images div\n",
    "                if 'id' in attrs_keys:\n",
    "                    # additional images\n",
    "                    if tag.attrs['id'] == 'jstree_thumb':\n",
    "                        # extract image url from within href of a\n",
    "                        img_links = [f'http://www.siamensis.org{x[\"href\"]}' for x in tag.find_all('a', attrs={'class': 'si-image'})]\n",
    "                        # caption is from alt of img \n",
    "                        captions = [f'{x[\"alt\"]}' for x in tag.find_all('img', attrs={'class': 'image-item'})]\n",
    "                        # construct image array with obj of images\n",
    "                        images = [{\n",
    "                            'url': k,\n",
    "                            'caption': v\n",
    "                        } for k,v in zip(img_links, captions)]\n",
    "                        \n",
    "                        # print(images)\n",
    "                        break\n",
    "            try:\n",
    "                # if none of above, this is part of a description\n",
    "                # add it to content\n",
    "                content += tag.prettify()\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "    # finally return content and images which can be empty\n",
    "    return (content, images)\n",
    "                                    \n",
    "def get_parent_list(node_soup):\n",
    "     # list of parents\n",
    "    tmp = node_soup.find(attrs={'type': 'text/javascript'}).text\n",
    "    parent_pt = re.compile(r\"jQ.parseJSON\\('(.*)'\\).*\")  \n",
    "    parent_list = parent_pt.search(tmp).group(1)\n",
    "    # turn list that is parsed as string into a list\n",
    "    parent_list = ast.literal_eval(parent_list)\n",
    "    # remove all white space\n",
    "    parent_list = [x.strip() for x in parent_list]\n",
    "    # remove itself from the list\n",
    "    parent_list.remove(each['id'])        \n",
    "    \n",
    "    return parent_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = glob('../node/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "copied_extracted_node = deepcopy(extracted_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6510/6510 [00:24<00:00, 265.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE at: 6510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pre-allocate counter and list for collecting all nodes\n",
    "counter = 0\n",
    "si_collection = []\n",
    "\n",
    "for each in tqdm(copied_extracted_node):\n",
    "    # pre-allocate object\n",
    "    si_node = {\n",
    "        'id': '',\n",
    "        'rank': '',\n",
    "        'title': '',\n",
    "        'author': '',\n",
    "        'timestamp': '',\n",
    "        'description': '',\n",
    "        'images': [],\n",
    "        'parents': [],\n",
    "    }\n",
    "    \n",
    "    ## ---- ID\n",
    "    si_node['id'] = get_id(each)\n",
    "    \n",
    "    ## ---- opening the file\n",
    "    # file name of each html\n",
    "    file_name = f\"../node/{each['id']}.html\"\n",
    "    # soup of each html\n",
    "    soup = BeautifulSoup(open(file_name, \"r\"), \"html.parser\")\n",
    "\n",
    "    ## ---- RANK AND TITLE\n",
    "    # get rank and rank name\n",
    "    node_rank, node_title = get_rank_and_title(soup)\n",
    "        \n",
    "    if not node_rank or not node_title:\n",
    "        break\n",
    "        \n",
    "    # assign to a node\n",
    "    si_node['rank'] = node_rank\n",
    "    si_node['title'] = node_title\n",
    "\n",
    "    ## ---- AUTHOR AND TIMESTAMP\n",
    "    node_author, node_timestamp = get_author_and_timestamp(soup)\n",
    "    if not node_author or not node_timestamp:\n",
    "        break\n",
    "    \n",
    "    # assign to a node\n",
    "    si_node['author'] = node_author\n",
    "    si_node['timestamp'] = node_timestamp.strftime(\"%d %b %Y, %H:%M\")\n",
    "           \n",
    "    ## ---- DESCRIPTION AND IMAGES\n",
    "    # get description and images\n",
    "    node_content, node_images = get_description_and_images(soup)\n",
    "    \n",
    "    # assign to a node\n",
    "    si_node['description'] = node_content\n",
    "    si_node['images'] = node_images\n",
    "    \n",
    "    ## ---- PARENT LIST\n",
    "    # assign to a node\n",
    "    node_parent_list = get_parent_list(soup)\n",
    "    si_node['parents'] = node_parent_list\n",
    "                              \n",
    "    ## ---- add to collection\n",
    "    # add to list and count success \n",
    "    si_collection.append(si_node)\n",
    "    counter += 1\n",
    "                                    \n",
    "print(f'DONE at: {counter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add number of children to each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloned_collection = deepcopy(si_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6510/6510 [00:40<00:00, 162.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, node in enumerate(tqdm(cloned_collection)):\n",
    "    tmp_id = node['id']\n",
    "    tmp = list(map(lambda x: tmp_id in x['parents'], si_collection))\n",
    "    \n",
    "    cloned_collection[idx]['num_children'] = tmp.count(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = path.join(path.pardir, 'si-data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'w') as f:\n",
    "    json.dump(cloned_collection, f, sort_keys=True, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
